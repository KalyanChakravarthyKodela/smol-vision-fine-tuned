{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Multimodal Granite 3.1 2B\n",
        "\n",
        "IBM released Granite-Vision-3.1-2B preview, a compact Llava-like vision language model based on Granite Instruct 3.1 for text backbone and SigLIP for image backbone.\n",
        "\n",
        "It has very impressive [scores on different benchmarks](https://huggingface.co/ibm-granite/granite-vision-3.1-2b-preview#granite-vision-31-2b-preview) for it's size for vision understanding and document understanding.\n",
        "\n",
        "It comes with transformers and vLLM integration from the start too! Let's put it to test."
      ],
      "metadata": {
        "id": "WMz0BjxhoNw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pdf2image git+https://github.com/huggingface/transformers.git\n",
        "!sudo apt-get install -q poppler-utils"
      ],
      "metadata": {
        "id": "ZX1NfV26o3-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://www.europarl.europa.eu/pdfs/news/expert/2018/7/story/20180706STO07407/20180706STO07407_en.pdf"
      ],
      "metadata": {
        "id": "2-RL91i2oitN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "\n",
        "pdf_path = \"20180706STO07407_en.pdf\"\n",
        "images = convert_from_path(pdf_path)"
      ],
      "metadata": {
        "id": "e4T7V9DlotQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'd like to ask Granite Vision to explain a chart, and perhaps ask further questions."
      ],
      "metadata": {
        "id": "B9g8C9qrqaEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images[1]"
      ],
      "metadata": {
        "id": "WimXEVTaqVtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use `LlavaNextForConditionalGeneration` class to load Granite Vision and infer."
      ],
      "metadata": {
        "id": "1HrTfrstq0K4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKs2jJ2pnn3r"
      },
      "outputs": [],
      "source": [
        "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
        "\n",
        "model_path = \"ibm-granite/granite-vision-3.1-2b-preview\"\n",
        "processor = LlavaNextProcessor.from_pretrained(model_path)\n",
        "model = LlavaNextForConditionalGeneration.from_pretrained(model_path, device_map=\"cuda:0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use chat template to format our image and text input together, then pass it to the model."
      ],
      "metadata": {
        "id": "OrhWVIjKq_JE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"url\": images[1]},\n",
        "            {\"type\": \"text\", \"text\": \"Explain the chart in the image in detail.\"},\n",
        "        ],\n",
        "    },\n",
        "]\n",
        "inputs = processor.apply_chat_template(\n",
        "    conversation,\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=True,\n",
        "    return_dict=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda:0\")\n",
        "\n",
        "\n",
        "# autoregressively complete prompt\n",
        "output = model.generate(**inputs, max_new_tokens=500)\n",
        "print(processor.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "ugGgPwT1n35l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The authors report various benchmarks, one that caught my eye was LiveXiv which is about ArXiv paper understanding. Let's put it to test."
      ],
      "metadata": {
        "id": "-W5Cf77Cuqxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://arxiv.org/pdf/2405.04324"
      ],
      "metadata": {
        "id": "mxQb8ZP6umb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/2405.04324\"\n",
        "images = convert_from_path(pdf_path)"
      ],
      "metadata": {
        "id": "iOafO9xRup62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll test understanding from a page of Granite Code paper of IBM."
      ],
      "metadata": {
        "id": "6ozQkvW1vGR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images[5]"
      ],
      "metadata": {
        "id": "TK5k3F5Ku75N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"url\": images[5]},\n",
        "            {\"type\": \"text\", \"text\": \"What differences does this paper contribute to model architecture and training?\"},\n",
        "        ],\n",
        "    },\n",
        "]\n",
        "inputs = processor.apply_chat_template(\n",
        "    conversation,\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=True,\n",
        "    return_dict=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda:0\")\n",
        "\n",
        "\n",
        "# autoregressively complete prompt\n",
        "output = model.generate(**inputs, max_new_tokens=500)\n",
        "print(processor.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "wIuN3O0svFNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The answer is correct!"
      ],
      "metadata": {
        "id": "7WiI9KYqvrqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get more info about the model from [the model repository](https://huggingface.co/ibm-granite/granite-vision-3.1-2b-preview), including benchmarks, how to get started with transformers and vLLM."
      ],
      "metadata": {
        "id": "6gRDmZyBwCi8"
      }
    }
  ]
}